{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install sentencepiece","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"rmV9AhhpDOTd","outputId":"9606a551-bbfe-44a5-da4c-383227a114d3","execution":{"iopub.status.busy":"2023-03-11T04:27:08.944147Z","iopub.execute_input":"2023-03-11T04:27:08.944523Z","iopub.status.idle":"2023-03-11T04:27:29.970948Z","shell.execute_reply.started":"2023-03-11T04:27:08.944489Z","shell.execute_reply":"2023-03-11T04:27:29.969672Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.26.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.97)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Importing the libraries needed\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport seaborn as sns\nimport transformers\nimport json\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaModel, RobertaTokenizer\nimport logging\nlogging.basicConfig(level=logging.ERROR)","metadata":{"id":"8k9rh56sDOTf","execution":{"iopub.status.busy":"2023-03-11T04:27:48.540591Z","iopub.execute_input":"2023-03-11T04:27:48.541269Z","iopub.status.idle":"2023-03-11T04:27:48.548383Z","shell.execute_reply.started":"2023-03-11T04:27:48.541227Z","shell.execute_reply":"2023-03-11T04:27:48.546784Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForMaskedLM\n\nfrom transformers import AutoTokenizer, AutoModel\nimport re\n\ntokenizer4 = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\ntokenizer5 = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")","metadata":{"id":"yofPBe_PDOTh","outputId":"ebe9f3e9-db74-48a7-abf4-e3cef5ad3a15","colab":{"referenced_widgets":["e7306048356044b8b32b11e7501b8823","d766741da01a4a08959c3c9257ff641a","323971e19c63437890966e2a33618071","583e03d0713a4948b566c73a39b527b6","a8ad7adfc5364b3e87bfb9ee95051d28","ab06a196c1d94dafaefa5487b4b941c5","7d3b0728bd344205894baf13803219b5"]},"execution":{"iopub.status.busy":"2023-03-11T04:27:51.954996Z","iopub.execute_input":"2023-03-11T04:27:51.955863Z","iopub.status.idle":"2023-03-11T04:28:00.636905Z","shell.execute_reply.started":"2023-03-11T04:27:51.955819Z","shell.execute_reply":"2023-03-11T04:28:00.635825Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f583959a38b84548a7957fd411261165"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76e2f1d5ba1c4bfea287063b300fd3f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09f8c0852afa462a8c1b7d23f881873b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07d86e9b5ef64446be15463707039194"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8972b5d31c85461f9c63d9446caa6ceb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e06a007007e04b01ac0b99e8eff0a20d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8838a48dfdf45cd83659d071d434a09"}},"metadata":{}}]},{"cell_type":"code","source":"# Setting up the device for GPU usage\n\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'","metadata":{"id":"WUQIKqD9DOTh","execution":{"iopub.status.busy":"2023-03-11T04:28:04.917733Z","iopub.execute_input":"2023-03-11T04:28:04.918212Z","iopub.status.idle":"2023-03-11T04:28:04.989992Z","shell.execute_reply.started":"2023-03-11T04:28:04.918169Z","shell.execute_reply":"2023-03-11T04:28:04.988830Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"id":"kt9rbQ6WDOTj","execution":{"iopub.status.busy":"2023-03-11T04:28:07.877773Z","iopub.execute_input":"2023-03-11T04:28:07.878173Z","iopub.status.idle":"2023-03-11T04:28:07.883180Z","shell.execute_reply.started":"2023-03-11T04:28:07.878137Z","shell.execute_reply":"2023-03-11T04:28:07.881867Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import random\nseed_val=3407\ntorch.manual_seed(seed_val)\nnp.random.seed(seed_val)\nrandom.seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"id":"ifO-FqUMDOTj","execution":{"iopub.status.busy":"2023-03-11T04:28:11.247106Z","iopub.execute_input":"2023-03-11T04:28:11.247702Z","iopub.status.idle":"2023-03-11T04:28:11.259800Z","shell.execute_reply.started":"2023-03-11T04:28:11.247658Z","shell.execute_reply":"2023-03-11T04:28:11.258538Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Load whole datasets\ndf_train = pd.read_csv(\"/kaggle/input/ipo-wholedataset/IPO_WholeDataset_ClassCreate.csv\", names = [\"tweet\", \"classes\"])","metadata":{"id":"gWgTHB7CDOTj","execution":{"iopub.status.busy":"2023-03-11T04:28:14.615661Z","iopub.execute_input":"2023-03-11T04:28:14.616794Z","iopub.status.idle":"2023-03-11T04:28:14.654431Z","shell.execute_reply.started":"2023-03-11T04:28:14.616753Z","shell.execute_reply":"2023-03-11T04:28:14.653378Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.iloc[1: , :]\ndf_train.head()","metadata":{"id":"86prgPq3DOTk","outputId":"377781bc-31ee-49ad-c999-1f0b6edbbe78","execution":{"iopub.status.busy":"2023-03-11T04:28:17.239895Z","iopub.execute_input":"2023-03-11T04:28:17.240639Z","iopub.status.idle":"2023-03-11T04:28:17.258881Z","shell.execute_reply.started":"2023-03-11T04:28:17.240595Z","shell.execute_reply":"2023-03-11T04:28:17.257760Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                               tweet classes\n1  Millions of Indians investing in the country’s...     0.0\n2  టోకు ద్రవ్యోల్బణమే కీలకం  via తాజా వార్తలు  | ...     1.0\n3  મેરે રે કો એસે ધક ધક હો રહા હે....\\nfingers cr...     1.0\n4  The country's largest insurer will list itself...     1.0\n5  Can a full time employee ask for board of dire...     1.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>classes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Millions of Indians investing in the country’s...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>టోకు ద్రవ్యోల్బణమే కీలకం  via తాజా వార్తలు  | ...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>મેરે રે કો એસે ધક ધક હો રહા હે....\\nfingers cr...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The country's largest insurer will list itself...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Can a full time employee ask for board of dire...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_train.describe()","metadata":{"id":"BsZp09AeDOTk","outputId":"50bdef81-03a4-4421-9c15-06fe86e46854","execution":{"iopub.status.busy":"2023-03-11T04:28:20.632682Z","iopub.execute_input":"2023-03-11T04:28:20.633112Z","iopub.status.idle":"2023-03-11T04:28:20.657088Z","shell.execute_reply.started":"2023-03-11T04:28:20.633050Z","shell.execute_reply":"2023-03-11T04:28:20.656001Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                                    tweet classes\ncount                                                 960     960\nunique                                                944       3\ntop     Kaun kaun god se prey kar rahe ki unko LICIPO ...     1.0\nfreq                                                    3     657","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>classes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>960</td>\n      <td>960</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>944</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Kaun kaun god se prey kar rahe ki unko LICIPO ...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>3</td>\n      <td>657</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Defining some key variables that will be used later on in the training\nMAX_LEN = 256\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 4\nLEARNING_RATE = 1e-08","metadata":{"id":"0dAS7OHQDOTl","execution":{"iopub.status.busy":"2023-03-11T04:28:25.029827Z","iopub.execute_input":"2023-03-11T04:28:25.030553Z","iopub.status.idle":"2023-03-11T04:28:25.035406Z","shell.execute_reply.started":"2023-03-11T04:28:25.030513Z","shell.execute_reply":"2023-03-11T04:28:25.034301Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class SentimentData(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.text = dataframe.tweet\n        #self.targets = self.data.class\n        # using loop\n        #y=list(df_train['class'])\n        y=list(dataframe.classes)\n        res = [eval(i) for i in y]\n        print(\"Modified list is: \", res)\n        yInt = [int(res) for res in res]\n        print(\"final->\",yInt)\n        self.targets = yInt\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = str(self.text[index])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n            #'targets': torch.tensor(self.targets[index], dtype=torch.str)\n        }","metadata":{"id":"jgG4NGpVDOTn","execution":{"iopub.status.busy":"2023-03-11T04:28:28.078366Z","iopub.execute_input":"2023-03-11T04:28:28.078739Z","iopub.status.idle":"2023-03-11T04:28:28.090117Z","shell.execute_reply.started":"2023-03-11T04:28:28.078705Z","shell.execute_reply":"2023-03-11T04:28:28.089091Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_size = 0.6\ntrain_data=df_train.sample(frac=train_size,random_state=seed_val)\ntest_data=df_train.drop(train_data.index).reset_index(drop=True)\ntrain_data = train_data.reset_index(drop=True)\n\n\nprint(\"FULL Dataset: {}\".format(df_train.shape))\nprint(\"TRAIN Dataset: {}\".format(train_data.shape))\nprint(\"TEST Dataset: {}\".format(test_data.shape))\n\ntraining_set4 = SentimentData(train_data, tokenizer4, MAX_LEN)\ntesting_set4 = SentimentData(test_data, tokenizer4, MAX_LEN)\n\ntraining_set5 = SentimentData(train_data, tokenizer5, MAX_LEN)\ntesting_set5 = SentimentData(test_data, tokenizer5, MAX_LEN)","metadata":{"id":"3zUGn_0iDOTo","outputId":"313332c3-95b5-45f8-b778-86e3018ab615","execution":{"iopub.status.busy":"2023-03-11T04:28:31.999263Z","iopub.execute_input":"2023-03-11T04:28:31.999641Z","iopub.status.idle":"2023-03-11T04:28:32.023386Z","shell.execute_reply.started":"2023-03-11T04:28:31.999605Z","shell.execute_reply":"2023-03-11T04:28:32.022134Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"FULL Dataset: (960, 2)\nTRAIN Dataset: (576, 2)\nTEST Dataset: (384, 2)\nModified list is:  [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 0.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]\nfinal-> [0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 2, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 2, 0, 0, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 2, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 0, 1, 2, 0, 0, 1, 2, 1, 1, 1, 0, 2, 0, 1, 1, 1, 0, 2, 0, 1, 0, 0, 1, 2, 1, 1, 2, 1, 0, 1, 0, 1, 1, 1, 1, 2, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, 0, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 2, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1]\nModified list is:  [0.0, 1.0, 2.0, 1.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\nfinal-> [0, 1, 2, 1, 1, 0, 2, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 0, 2, 0, 1, 0, 2, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 2, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 0, 2, 1, 1, 1, 0, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 2, 1, 2, 0, 1, 2, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 2, 2, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1, 2, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\nModified list is:  [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 0.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]\nfinal-> [0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 2, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 2, 0, 0, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 2, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 0, 1, 2, 0, 0, 1, 2, 1, 1, 1, 0, 2, 0, 1, 1, 1, 0, 2, 0, 1, 0, 0, 1, 2, 1, 1, 2, 1, 0, 1, 0, 1, 1, 1, 1, 2, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, 0, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 2, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1]\nModified list is:  [0.0, 1.0, 2.0, 1.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\nfinal-> [0, 1, 2, 1, 1, 0, 2, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 0, 2, 0, 1, 0, 2, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 2, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 0, 2, 1, 1, 1, 0, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 2, 1, 2, 0, 1, 2, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 2, 2, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1, 2, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': False,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': False,\n                'num_workers': 0\n                }\n\ntraining_loader4 = DataLoader(training_set4, **train_params)\ntesting_loader4 = DataLoader(testing_set4, **test_params)\n\ntraining_loader5 = DataLoader(training_set5, **train_params)\ntesting_loader5 = DataLoader(testing_set5, **test_params)\n","metadata":{"id":"9f3qKGhwDOTp","execution":{"iopub.status.busy":"2023-03-11T04:28:38.367307Z","iopub.execute_input":"2023-03-11T04:28:38.367742Z","iopub.status.idle":"2023-03-11T04:28:38.375806Z","shell.execute_reply.started":"2023-03-11T04:28:38.367706Z","shell.execute_reply":"2023-03-11T04:28:38.374618Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"weights = [.762, .316, .923]\nclass_weights=torch.tensor(weights,dtype=torch.float)\nloss_function = torch.nn.CrossEntropyLoss(weight = class_weights)","metadata":{"id":"s9LOm2F9DOTp","execution":{"iopub.status.busy":"2023-03-11T04:28:41.342793Z","iopub.execute_input":"2023-03-11T04:28:41.343784Z","iopub.status.idle":"2023-03-11T04:28:41.363508Z","shell.execute_reply.started":"2023-03-11T04:28:41.343726Z","shell.execute_reply":"2023-03-11T04:28:41.362304Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class mBERTClass(torch.nn.Module):\n    def __init__(self):\n        super(mBERTClass, self).__init__()\n        self.l1 = AutoModelForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")\n        #self.pre_classifier = torch.nn.Linear(768, 768)\n        self.pre_classifier = torch.nn.Linear(119547, 68)\n        self.dropout = torch.nn.Dropout(0.3)\n        self.classifier = torch.nn.Linear(68, 3)\n        self.loss_fn = loss_function\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        hidden_state = output_1[0]\n        pooler = hidden_state[:, 0]\n        pooler = self.pre_classifier(pooler)\n        pooler = torch.nn.ReLU()(pooler)\n        pooler = self.dropout(pooler)\n        output = self.classifier(pooler)\n        return output","metadata":{"id":"qFTAQtRMDOTq","execution":{"iopub.status.busy":"2023-03-11T04:28:44.182726Z","iopub.execute_input":"2023-03-11T04:28:44.183308Z","iopub.status.idle":"2023-03-11T04:28:44.192001Z","shell.execute_reply.started":"2023-03-11T04:28:44.183266Z","shell.execute_reply":"2023-03-11T04:28:44.190544Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class XLMRobertaClass(torch.nn.Module):\n    def __init__(self):\n        super(XLMRobertaClass, self).__init__()\n        self.l1 = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")\n        #self.pre_classifier = torch.nn.Linear(768, 768)\n        self.pre_classifier = torch.nn.Linear(250002, 68)\n        self.dropout = torch.nn.Dropout(0.3)\n        self.classifier = torch.nn.Linear(68, 3)\n        self.loss_fn = loss_function\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        hidden_state = output_1[0]\n        pooler = hidden_state[:, 0]\n        pooler = self.pre_classifier(pooler)\n        pooler = torch.nn.ReLU()(pooler)\n        pooler = self.dropout(pooler)\n        output = self.classifier(pooler)\n        return output","metadata":{"id":"KEw1bqGbDOTq","execution":{"iopub.status.busy":"2023-03-11T04:28:47.687506Z","iopub.execute_input":"2023-03-11T04:28:47.688049Z","iopub.status.idle":"2023-03-11T04:28:47.706451Z","shell.execute_reply.started":"2023-03-11T04:28:47.687988Z","shell.execute_reply":"2023-03-11T04:28:47.704871Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model4 = mBERTClass()\nmodel4.to(device)\n\nmodel5 = XLMRobertaClass()\nmodel5.to(device)","metadata":{"id":"CkeIhkKeDOTq","outputId":"31fc3bac-f584-45a3-9670-0a81719d519b","colab":{"referenced_widgets":["94b7993d83164b8ca367ea4ba107687c","acf3460b439747e0b105c76768430783"]},"execution":{"iopub.status.busy":"2023-03-11T04:28:51.488396Z","iopub.execute_input":"2023-03-11T04:28:51.489328Z","iopub.status.idle":"2023-03-11T04:29:18.049158Z","shell.execute_reply.started":"2023-03-11T04:28:51.489271Z","shell.execute_reply":"2023-03-11T04:29:18.046887Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee8ac54a27f3492ca38c54be2aea8f96"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c166de29ea1447daa09c51e23ca9fa41"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"XLMRobertaClass(\n  (l1): XLMRobertaForMaskedLM(\n    (roberta): XLMRobertaModel(\n      (embeddings): XLMRobertaEmbeddings(\n        (word_embeddings): Embedding(250002, 768, padding_idx=1)\n        (position_embeddings): Embedding(514, 768, padding_idx=1)\n        (token_type_embeddings): Embedding(1, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): XLMRobertaEncoder(\n        (layer): ModuleList(\n          (0): XLMRobertaLayer(\n            (attention): XLMRobertaAttention(\n              (self): XLMRobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): XLMRobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): XLMRobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): XLMRobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): XLMRobertaLayer(\n            (attention): XLMRobertaAttention(\n              (self): XLMRobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): XLMRobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): XLMRobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): XLMRobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (2): XLMRobertaLayer(\n            (attention): XLMRobertaAttention(\n              (self): XLMRobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): XLMRobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): XLMRobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): XLMRobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (3): XLMRobertaLayer(\n            (attention): XLMRobertaAttention(\n              (self): XLMRobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): XLMRobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): XLMRobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): XLMRobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (4): XLMRobertaLayer(\n            (attention): XLMRobertaAttention(\n              (self): XLMRobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): XLMRobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): XLMRobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): XLMRobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (5): XLMRobertaLayer(\n            (attention): XLMRobertaAttention(\n              (self): XLMRobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): XLMRobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): XLMRobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): XLMRobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (6): XLMRobertaLayer(\n            (attention): XLMRobertaAttention(\n              (self): XLMRobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): XLMRobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): XLMRobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): XLMRobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (7): XLMRobertaLayer(\n            (attention): XLMRobertaAttention(\n              (self): XLMRobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): XLMRobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): XLMRobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): XLMRobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (8): XLMRobertaLayer(\n            (attention): XLMRobertaAttention(\n              (self): XLMRobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): XLMRobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): XLMRobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): XLMRobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (9): XLMRobertaLayer(\n            (attention): XLMRobertaAttention(\n              (self): XLMRobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): XLMRobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): XLMRobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): XLMRobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (10): XLMRobertaLayer(\n            (attention): XLMRobertaAttention(\n              (self): XLMRobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): XLMRobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): XLMRobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): XLMRobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (11): XLMRobertaLayer(\n            (attention): XLMRobertaAttention(\n              (self): XLMRobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): XLMRobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): XLMRobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): XLMRobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (lm_head): XLMRobertaLMHead(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (decoder): Linear(in_features=768, out_features=250002, bias=True)\n    )\n  )\n  (pre_classifier): Linear(in_features=250002, out_features=68, bias=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (classifier): Linear(in_features=68, out_features=3, bias=True)\n  (loss_fn): CrossEntropyLoss()\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Creating optimizer\noptimizer4 = torch.optim.SGD(params =  model4.parameters(), lr=LEARNING_RATE)\noptimizer5 = torch.optim.SGD(params =  model5.parameters(), lr=LEARNING_RATE)","metadata":{"id":"tzhg-B5yDOTr","execution":{"iopub.status.busy":"2023-03-11T04:29:27.839209Z","iopub.execute_input":"2023-03-11T04:29:27.839592Z","iopub.status.idle":"2023-03-11T04:29:27.847497Z","shell.execute_reply.started":"2023-03-11T04:29:27.839560Z","shell.execute_reply":"2023-03-11T04:29:27.846116Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# For Model 4 , pair 1 to be updated\npredicted14 = []\noriginal14=[]\n# For Model 5 , pair 1 to be updated\npredicted15 = []\noriginal15=[]\n\ndef calcuate_accuracy(preds, targets, number):\n    if number==1:\n        predicted11.append(preds.tolist())\n        original11.append(targets.tolist())\n        n_correct1 = (preds==targets).sum().item()\n        return n_correct1\n    if number==2:\n        predicted12.append(preds.tolist())\n        original12.append(targets.tolist())\n        n_correct2 = (preds==targets).sum().item()\n        return n_correct2\n    if number==3:\n        predicted13.append(preds.tolist())\n        original13.append(targets.tolist())\n        n_correct3 = (preds==targets).sum().item()\n        return n_correct3\n    if number==4:\n        predicted14.append(preds.tolist())\n        original14.append(targets.tolist())\n        n_correct4 = (preds==targets).sum().item()\n        return n_correct4\n    if number==5:\n        predicted15.append(preds.tolist())\n        original15.append(targets.tolist())\n        n_correct5 = (preds==targets).sum().item()\n        return n_correct5","metadata":{"id":"PL4VE5_pDOTr","execution":{"iopub.status.busy":"2023-03-11T04:29:30.719245Z","iopub.execute_input":"2023-03-11T04:29:30.720216Z","iopub.status.idle":"2023-03-11T04:29:30.732594Z","shell.execute_reply.started":"2023-03-11T04:29:30.720172Z","shell.execute_reply":"2023-03-11T04:29:30.731284Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def calcuate_accuracyTrain(preds, targets):\n    n_correct = (preds==targets).sum().item()\n    #print(\"Inside accuracyTrain() method\")\n    return n_correct","metadata":{"id":"4mNx-UqgDOTr","execution":{"iopub.status.busy":"2023-03-11T04:29:35.118210Z","iopub.execute_input":"2023-03-11T04:29:35.118626Z","iopub.status.idle":"2023-03-11T04:29:35.124392Z","shell.execute_reply.started":"2023-03-11T04:29:35.118589Z","shell.execute_reply":"2023-03-11T04:29:35.123134Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def train(model, optimizer, training_loader, epoch):\n    tr_loss = 0\n    n_correct = 0\n    nb_tr_steps = 0\n    nb_tr_examples = 0\n    model.train()\n    for _,data in tqdm(enumerate(training_loader, 0)):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.long)\n\n        outputs = model(ids, mask, token_type_ids)\n        loss = loss_function(outputs, targets)\n        tr_loss += loss.item()\n        big_val, big_idx = torch.max(outputs.data, dim=1)\n        n_correct += calcuate_accuracyTrain(big_idx, targets)\n\n        nb_tr_steps += 1\n        nb_tr_examples+=targets.size(0)\n        \n        if _%5000==0:\n            loss_step = tr_loss/nb_tr_steps\n            accu_step = (n_correct*100)/nb_tr_examples \n            #print(f\"Training Loss per 5000 steps: {loss_step}\")\n            #print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n\n        optimizer.zero_grad()\n        loss.backward()\n        # # When using GPU\n        optimizer.step()\n\n    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n    epoch_loss = tr_loss/nb_tr_steps\n    epoch_accu = (n_correct*100)/nb_tr_examples\n    print(f\"Training Loss Epoch: {epoch_loss}\")\n    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n\n    return ","metadata":{"id":"CEYiqVhGDOTs","execution":{"iopub.status.busy":"2023-03-11T04:29:37.871038Z","iopub.execute_input":"2023-03-11T04:29:37.871423Z","iopub.status.idle":"2023-03-11T04:29:37.884292Z","shell.execute_reply.started":"2023-03-11T04:29:37.871391Z","shell.execute_reply":"2023-03-11T04:29:37.883294Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 4\nprint(f\"Finetuning with FOURTH model:::\")\nfor epoch in range(EPOCHS):\n    train(model4, optimizer4, training_loader4, epoch)\nprint(f\"Finetuning with FIFTH model:::\")\nfor epoch in range(EPOCHS):\n    train(model5, optimizer5, training_loader5, epoch) ","metadata":{"id":"ebaEfZteDOTs","outputId":"000f59b8-12dd-45f3-fe98-a9e483ffffac","execution":{"iopub.status.busy":"2023-03-11T04:29:42.736474Z","iopub.execute_input":"2023-03-11T04:29:42.736844Z","iopub.status.idle":"2023-03-11T04:33:53.727818Z","shell.execute_reply.started":"2023-03-11T04:29:42.736812Z","shell.execute_reply":"2023-03-11T04:33:53.726855Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Finetuning with FOURTH model:::\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2345: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n72it [00:26,  2.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 0: 45.65972222222222\nTraining Loss Epoch: 1.1988943612409964\nTraining Accuracy Epoch: 45.65972222222222\n","output_type":"stream"},{"name":"stderr","text":"72it [00:25,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 1: 45.833333333333336\nTraining Loss Epoch: 1.2196904598838754\nTraining Accuracy Epoch: 45.833333333333336\n","output_type":"stream"},{"name":"stderr","text":"72it [00:25,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 2: 47.74305555555556\nTraining Loss Epoch: 1.2062861033611827\nTraining Accuracy Epoch: 47.74305555555556\n","output_type":"stream"},{"name":"stderr","text":"72it [00:25,  2.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 3: 47.74305555555556\nTraining Loss Epoch: 1.1665753713912435\nTraining Accuracy Epoch: 47.74305555555556\nFinetuning with FIFTH model:::\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n72it [00:37,  1.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 0: 30.90277777777778\nTraining Loss Epoch: 2.9840010156234107\nTraining Accuracy Epoch: 30.90277777777778\n","output_type":"stream"},{"name":"stderr","text":"72it [00:37,  1.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 1: 39.93055555555556\nTraining Loss Epoch: 2.4744975889722505\nTraining Accuracy Epoch: 39.93055555555556\n","output_type":"stream"},{"name":"stderr","text":"72it [00:37,  1.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 2: 44.96527777777778\nTraining Loss Epoch: 2.42523864739471\nTraining Accuracy Epoch: 44.96527777777778\n","output_type":"stream"},{"name":"stderr","text":"72it [00:37,  1.93it/s]","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 3: 44.791666666666664\nTraining Loss Epoch: 2.0818223588996463\nTraining Accuracy Epoch: 44.791666666666664\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"def valid(model, testing_loader, number):\n    model.eval()\n    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n    with torch.no_grad():\n        for _, data in tqdm(enumerate(testing_loader, 0)):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n            targets = data['targets'].to(device, dtype = torch.long)\n            outputs = model(ids, mask, token_type_ids).squeeze()\n            loss = loss_function(outputs, targets)\n            tr_loss += loss.item()\n            big_val, big_idx = torch.max(outputs.data, dim=1)\n            n_correct += calcuate_accuracy(big_idx, targets, number)\n\n            nb_tr_steps += 1\n            nb_tr_examples+=targets.size(0)\n            \n            if _%5000==0:\n                loss_step = tr_loss/nb_tr_steps\n                accu_step = (n_correct*100)/nb_tr_examples\n                #print(f\"Validation Loss per 100 steps: {loss_step}\")\n                #print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n    epoch_loss = tr_loss/nb_tr_steps\n    epoch_accu = (n_correct*100)/nb_tr_examples\n    print(f\"Validation Loss Epoch: {epoch_loss}\")\n    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n    \n    return epoch_accu","metadata":{"id":"20J-RZyFDOTs","execution":{"iopub.status.busy":"2023-03-11T04:34:30.153620Z","iopub.execute_input":"2023-03-11T04:34:30.154014Z","iopub.status.idle":"2023-03-11T04:34:30.164391Z","shell.execute_reply.started":"2023-03-11T04:34:30.153978Z","shell.execute_reply":"2023-03-11T04:34:30.163051Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"acc4 = valid(model4, testing_loader4, 4)\nprint(f\"Validation Accuracy FOURTH Model::::\\n\")\nprint(\"Accuracy on test data = %0.2f%%\" % acc4)\nacc5 = valid(model5, testing_loader5, 5)\nprint(f\"Validation Accuracy FIFTH Model::::\\n\")\nprint(\"Accuracy on test data = %0.2f%%\" % acc5)","metadata":{"id":"rbtI7NJmDOTt","outputId":"8ee17c39-d927-4765-f35c-38aea7f321ad","execution":{"iopub.status.busy":"2023-03-11T04:34:34.481174Z","iopub.execute_input":"2023-03-11T04:34:34.481546Z","iopub.status.idle":"2023-03-11T04:34:49.279640Z","shell.execute_reply.started":"2023-03-11T04:34:34.481512Z","shell.execute_reply":"2023-03-11T04:34:49.278487Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"96it [00:06, 15.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss Epoch: 1.063129232575496\nValidation Accuracy Epoch: 52.083333333333336\nValidation Accuracy FOURTH Model::::\n\nAccuracy on test data = 52.08%\n","output_type":"stream"},{"name":"stderr","text":"96it [00:08, 10.94it/s]","output_type":"stream"},{"name":"stdout","text":"Validation Loss Epoch: 1.2266359538771212\nValidation Accuracy Epoch: 66.92708333333333\nValidation Accuracy FIFTH Model::::\n\nAccuracy on test data = 66.93%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"***********************************************\")\nprint(len(predicted14))\nprint(predicted14)\nprint(\"***********************************************\")\nprint(len(predicted15))\nprint(predicted15)","metadata":{"id":"5OgeNIWFDOTt","outputId":"ba7e8339-71a9-4731-cc55-09c142c90ce8","execution":{"iopub.status.busy":"2023-03-11T04:34:56.802513Z","iopub.execute_input":"2023-03-11T04:34:56.803300Z","iopub.status.idle":"2023-03-11T04:34:56.810572Z","shell.execute_reply.started":"2023-03-11T04:34:56.803259Z","shell.execute_reply":"2023-03-11T04:34:56.809143Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"***********************************************\n96\n[[1, 1, 1, 0], [0, 0, 1, 1], [1, 0, 1, 1], [1, 0, 1, 1], [1, 0, 0, 0], [0, 1, 1, 1], [1, 1, 1, 0], [1, 1, 0, 0], [0, 0, 1, 1], [1, 0, 0, 1], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 1], [0, 1, 1, 1], [0, 1, 0, 0], [0, 1, 0, 0], [1, 1, 1, 0], [1, 1, 0, 0], [1, 1, 0, 1], [1, 0, 1, 0], [1, 0, 1, 1], [0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 1, 0], [0, 1, 1, 1], [1, 0, 1, 1], [1, 0, 1, 1], [0, 0, 0, 0], [1, 1, 1, 0], [0, 0, 1, 1], [0, 1, 1, 1], [0, 1, 1, 1], [1, 0, 0, 1], [1, 0, 1, 1], [1, 0, 0, 0], [1, 0, 0, 1], [0, 0, 0, 1], [1, 1, 0, 0], [1, 1, 0, 1], [1, 1, 1, 0], [1, 1, 0, 0], [1, 0, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 0, 0, 1], [0, 0, 0, 1], [0, 1, 0, 0], [1, 0, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 1], [1, 1, 1, 1], [0, 1, 0, 0], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 1, 0], [1, 1, 1, 0], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 0, 0], [1, 1, 0, 1], [1, 1, 0, 1], [1, 0, 1, 0], [1, 0, 0, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [0, 1, 1, 1], [1, 1, 0, 0], [0, 0, 0, 1], [0, 1, 0, 0], [1, 0, 1, 1], [0, 0, 0, 0], [1, 1, 1, 1], [1, 1, 1, 1], [1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 0, 1], [0, 0, 1, 1], [0, 1, 1, 1], [1, 1, 0, 0], [1, 1, 0, 1], [0, 1, 1, 1], [1, 1, 1, 1], [1, 1, 0, 0], [1, 1, 0, 0], [1, 0, 0, 1], [1, 0, 1, 1], [0, 1, 0, 0], [1, 0, 1, 0], [0, 1, 0, 0], [1, 1, 1, 0], [1, 1, 0, 0], [0, 1, 1, 0]]\n***********************************************\n96\n[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 0, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"**************************************\")\nprint(len(original14))\nprint(original14)\nprint(\"**************************************\")\nprint(len(original15))\nprint(original15)\n","metadata":{"id":"evUP43AyDOTu","outputId":"b3862c96-3609-41bd-8035-1b491125a360","execution":{"iopub.status.busy":"2023-03-11T04:35:00.138895Z","iopub.execute_input":"2023-03-11T04:35:00.139596Z","iopub.status.idle":"2023-03-11T04:35:00.145938Z","shell.execute_reply.started":"2023-03-11T04:35:00.139556Z","shell.execute_reply":"2023-03-11T04:35:00.144331Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"**************************************\n96\n[[0, 1, 2, 1], [1, 0, 2, 0], [1, 0, 1, 1], [0, 1, 1, 1], [1, 1, 1, 2], [1, 1, 1, 0], [1, 1, 1, 0], [2, 0, 1, 0], [2, 0, 1, 0], [0, 1, 1, 0], [1, 1, 1, 0], [0, 0, 1, 1], [1, 1, 0, 1], [1, 1, 0, 1], [0, 0, 1, 1], [1, 0, 1, 0], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 1], [0, 1, 1, 0], [0, 1, 2, 1], [1, 0, 1, 0], [1, 1, 1, 1], [0, 1, 1, 1], [0, 2, 1, 1], [1, 1, 0, 1], [1, 0, 0, 1], [1, 0, 1, 1], [1, 1, 1, 0], [1, 1, 1, 1], [0, 1, 1, 0], [1, 1, 0, 0], [0, 0, 0, 1], [2, 1, 1, 1], [1, 0, 2, 1], [1, 1, 0, 1], [0, 2, 1, 1], [1, 1, 1, 1], [1, 1, 1, 2], [1, 1, 1, 0], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [2, 1, 1, 2], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 1, 1], [0, 0, 0, 1], [1, 2, 0, 1], [1, 1, 1, 1], [0, 0, 1, 1], [2, 1, 2, 0], [1, 2, 1, 0], [1, 1, 0, 0], [1, 0, 1, 1], [0, 1, 1, 1], [1, 1, 2, 2], [0, 1, 1, 2], [1, 1, 1, 1], [1, 1, 2, 1], [1, 1, 1, 2], [2, 1, 1, 1], [1, 1, 1, 1], [1, 0, 1, 0], [1, 2, 1, 1], [2, 1, 1, 0], [1, 1, 0, 2], [1, 1, 1, 2], [1, 1, 0, 1], [1, 0, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 0, 1, 1], [1, 2, 1, 0], [1, 1, 1, 1], [1, 2, 1, 2], [0, 0, 1, 1], [1, 0, 0, 1], [1, 1, 1, 0], [2, 1, 1, 1], [1, 1, 1, 1], [1, 0, 1, 1], [0, 1, 0, 1], [1, 1, 1, 1], [1, 1, 2, 0], [1, 1, 0, 0], [0, 1, 1, 1], [1, 0, 0, 1], [1, 0, 0, 1], [1, 1, 1, 1], [1, 2, 1, 1], [0, 0, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1]]\n**************************************\n96\n[[0, 1, 2, 1], [1, 0, 2, 0], [1, 0, 1, 1], [0, 1, 1, 1], [1, 1, 1, 2], [1, 1, 1, 0], [1, 1, 1, 0], [2, 0, 1, 0], [2, 0, 1, 0], [0, 1, 1, 0], [1, 1, 1, 0], [0, 0, 1, 1], [1, 1, 0, 1], [1, 1, 0, 1], [0, 0, 1, 1], [1, 0, 1, 0], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 1], [0, 1, 1, 0], [0, 1, 2, 1], [1, 0, 1, 0], [1, 1, 1, 1], [0, 1, 1, 1], [0, 2, 1, 1], [1, 1, 0, 1], [1, 0, 0, 1], [1, 0, 1, 1], [1, 1, 1, 0], [1, 1, 1, 1], [0, 1, 1, 0], [1, 1, 0, 0], [0, 0, 0, 1], [2, 1, 1, 1], [1, 0, 2, 1], [1, 1, 0, 1], [0, 2, 1, 1], [1, 1, 1, 1], [1, 1, 1, 2], [1, 1, 1, 0], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [2, 1, 1, 2], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 1, 1], [1, 1, 1, 0], [1, 1, 1, 1], [0, 0, 0, 1], [1, 2, 0, 1], [1, 1, 1, 1], [0, 0, 1, 1], [2, 1, 2, 0], [1, 2, 1, 0], [1, 1, 0, 0], [1, 0, 1, 1], [0, 1, 1, 1], [1, 1, 2, 2], [0, 1, 1, 2], [1, 1, 1, 1], [1, 1, 2, 1], [1, 1, 1, 2], [2, 1, 1, 1], [1, 1, 1, 1], [1, 0, 1, 0], [1, 2, 1, 1], [2, 1, 1, 0], [1, 1, 0, 2], [1, 1, 1, 2], [1, 1, 0, 1], [1, 0, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 0, 1, 1], [1, 2, 1, 0], [1, 1, 1, 1], [1, 2, 1, 2], [0, 0, 1, 1], [1, 0, 0, 1], [1, 1, 1, 0], [2, 1, 1, 1], [1, 1, 1, 1], [1, 0, 1, 1], [0, 1, 0, 1], [1, 1, 1, 1], [1, 1, 2, 0], [1, 1, 0, 0], [0, 1, 1, 1], [1, 0, 0, 1], [1, 0, 0, 1], [1, 1, 1, 1], [1, 2, 1, 1], [0, 0, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1]]\n","output_type":"stream"}]},{"cell_type":"code","source":"final_prediction4 = []\nfinal_prediction5 = []\nfor sublist in predicted14:\n    for item in sublist:\n        final_prediction4.append(item)\nfor sublist in predicted15:\n    for item in sublist:\n        final_prediction5.append(item)","metadata":{"id":"2ajYNA9HDOTu","execution":{"iopub.status.busy":"2023-03-11T04:35:03.183984Z","iopub.execute_input":"2023-03-11T04:35:03.185414Z","iopub.status.idle":"2023-03-11T04:35:03.191632Z","shell.execute_reply.started":"2023-03-11T04:35:03.185356Z","shell.execute_reply":"2023-03-11T04:35:03.190247Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"final_original4 = []\nfinal_original5 = []\nfor sublist in original14:\n    for item in sublist:\n        final_original4.append(item)\nfor sublist in original15:\n    for item in sublist:\n        final_original5.append(item)","metadata":{"id":"MnLbStXTDOTu","execution":{"iopub.status.busy":"2023-03-11T04:35:05.902647Z","iopub.execute_input":"2023-03-11T04:35:05.903588Z","iopub.status.idle":"2023-03-11T04:35:05.909904Z","shell.execute_reply.started":"2023-03-11T04:35:05.903537Z","shell.execute_reply":"2023-03-11T04:35:05.908800Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print(\"*****************************************\")\nprint(len(final_prediction4))\nprint(final_prediction4)\nprint(\"*****************************************\")\nprint(len(final_prediction5))\nprint(final_prediction5)","metadata":{"id":"QFp2XMNuDOTv","outputId":"dbb3d808-2f94-48d3-85c9-a2fe9c330ab5","execution":{"iopub.status.busy":"2023-03-11T04:35:09.024381Z","iopub.execute_input":"2023-03-11T04:35:09.024756Z","iopub.status.idle":"2023-03-11T04:35:09.031935Z","shell.execute_reply.started":"2023-03-11T04:35:09.024720Z","shell.execute_reply":"2023-03-11T04:35:09.030656Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"*****************************************\n384\n[1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0]\n*****************************************\n384\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"**************************\")\nprint(len(final_original4))\nprint(final_original4)\nprint(\"**************************\")\nprint(len(final_original5))\nprint(final_original5)","metadata":{"id":"HXlUlUp2DOTw","outputId":"9cbbe01a-9ec9-4ad1-d3b7-b3f975e5cecb","execution":{"iopub.status.busy":"2023-03-11T04:35:12.193156Z","iopub.execute_input":"2023-03-11T04:35:12.193871Z","iopub.status.idle":"2023-03-11T04:35:12.200197Z","shell.execute_reply.started":"2023-03-11T04:35:12.193829Z","shell.execute_reply":"2023-03-11T04:35:12.198948Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"**************************\n384\n[0, 1, 2, 1, 1, 0, 2, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 0, 2, 0, 1, 0, 2, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 2, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 0, 2, 1, 1, 1, 0, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 2, 1, 2, 0, 1, 2, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 2, 2, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1, 2, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n**************************\n384\n[0, 1, 2, 1, 1, 0, 2, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 0, 2, 0, 1, 0, 2, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 2, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 0, 2, 1, 1, 1, 0, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 2, 1, 2, 0, 1, 2, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 2, 2, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1, 2, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"import sklearn","metadata":{"id":"YtHvEvF_DOTw","execution":{"iopub.status.busy":"2023-03-11T04:35:16.935835Z","iopub.execute_input":"2023-03-11T04:35:16.936640Z","iopub.status.idle":"2023-03-11T04:35:16.941643Z","shell.execute_reply.started":"2023-03-11T04:35:16.936599Z","shell.execute_reply":"2023-03-11T04:35:16.940471Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"print(\"***********Confusion Matrix by MODEL 4 over Test Set**********************\")\nmat4 = sklearn.metrics.confusion_matrix(final_original4,final_prediction4)\nmat4","metadata":{"id":"oNu6oN0ODOTx","outputId":"efc95b2e-3570-4281-a10a-6ceedd8b1d1e","execution":{"iopub.status.busy":"2023-03-11T04:35:19.969361Z","iopub.execute_input":"2023-03-11T04:35:19.970323Z","iopub.status.idle":"2023-03-11T04:35:19.982517Z","shell.execute_reply.started":"2023-03-11T04:35:19.970270Z","shell.execute_reply":"2023-03-11T04:35:19.981355Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"***********Confusion Matrix by MODEL 4 over Test Set**********************\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"array([[ 40,  53,   0],\n       [ 98, 160,   0],\n       [  9,  24,   0]])"},"metadata":{}}]},{"cell_type":"code","source":"print(\"***********Confusion Matrix by MODEL 5 over Test Set**********************\")\nmat5 = sklearn.metrics.confusion_matrix(final_original5,final_prediction5)\nmat5","metadata":{"id":"WoyfSFjNDOTx","outputId":"8cdad3c8-ce54-4de2-ade6-4c344f2459ed","execution":{"iopub.status.busy":"2023-03-11T04:35:25.321926Z","iopub.execute_input":"2023-03-11T04:35:25.322648Z","iopub.status.idle":"2023-03-11T04:35:25.333051Z","shell.execute_reply.started":"2023-03-11T04:35:25.322608Z","shell.execute_reply":"2023-03-11T04:35:25.331748Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"***********Confusion Matrix by MODEL 5 over Test Set**********************\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"array([[  0,  93,   0],\n       [  1, 257,   0],\n       [  1,  32,   0]])"},"metadata":{}}]},{"cell_type":"code","source":"print(\"***********Accuracy by MODEL 4 over Test Set**********************\")\nsklearn.metrics.accuracy_score(final_original4,final_prediction4)","metadata":{"id":"eRQx8FVGDOTx","outputId":"d22519f6-ead3-4892-ae0e-9990a583a097","execution":{"iopub.status.busy":"2023-03-11T04:35:28.769889Z","iopub.execute_input":"2023-03-11T04:35:28.770267Z","iopub.status.idle":"2023-03-11T04:35:28.780632Z","shell.execute_reply.started":"2023-03-11T04:35:28.770232Z","shell.execute_reply":"2023-03-11T04:35:28.779402Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"***********Accuracy by MODEL 4 over Test Set**********************\n","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"0.5208333333333334"},"metadata":{}}]},{"cell_type":"code","source":"print(\"***********Accuracy by MODEL 5 over Test Set**********************\")\nsklearn.metrics.accuracy_score(final_original5,final_prediction5)","metadata":{"id":"hup6D4HSDOTx","outputId":"422721a5-db06-4a99-8a2d-3bfe7305eb92","execution":{"iopub.status.busy":"2023-03-11T04:35:31.665042Z","iopub.execute_input":"2023-03-11T04:35:31.666094Z","iopub.status.idle":"2023-03-11T04:35:31.674637Z","shell.execute_reply.started":"2023-03-11T04:35:31.666033Z","shell.execute_reply":"2023-03-11T04:35:31.673493Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"***********Accuracy by MODEL 5 over Test Set**********************\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"0.6692708333333334"},"metadata":{}}]},{"cell_type":"code","source":"print(len(final_prediction4))\nwith open(\"final_prediction4_S2_U.txt\", \"w\") as output:\n    output.write(str(final_prediction4))","metadata":{"id":"yXxClFOEDOTy","outputId":"e848c049-db23-40a8-b0a1-354657836ba8","execution":{"iopub.status.busy":"2023-03-11T04:35:53.418567Z","iopub.execute_input":"2023-03-11T04:35:53.419373Z","iopub.status.idle":"2023-03-11T04:35:53.425418Z","shell.execute_reply.started":"2023-03-11T04:35:53.419330Z","shell.execute_reply":"2023-03-11T04:35:53.424348Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"384\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(final_prediction5))\nwith open(\"final_prediction5_S2_U.txt\", \"w\") as output:\n    output.write(str(final_prediction5))","metadata":{"id":"Ip8gekv-DOTy","outputId":"dbf9a903-9457-4c9f-ed7f-fa040771dc63","execution":{"iopub.status.busy":"2023-03-11T04:35:55.730133Z","iopub.execute_input":"2023-03-11T04:35:55.730709Z","iopub.status.idle":"2023-03-11T04:35:55.737216Z","shell.execute_reply.started":"2023-03-11T04:35:55.730671Z","shell.execute_reply":"2023-03-11T04:35:55.736030Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"384\n","output_type":"stream"}]}]}